Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2019-07-24T05:47:42+02:00

====== Videos ======
Created Mittwoch 24 Juli 2019


===== Create (and modify) videos =====
Using **ffmpeg**
* ''$ ffmpeg -framerate 24 -i %03d.png output.mp4''
	  --> ''-framerate 24''		default is 25 frames per secons
	  --> ''-i '%03d.png'''		specify input files (* does NOT work)
* ''$ ffmpeg -start_number 42 -i '%30d.png' -c:v libx264 out.mp4''
	  --> ''-start_number 42''	default is 0 (can be omitted then)
	  --> ''-c:v libx264''
	  --> ''out.mp4''
* png files use RGB color space, conversion to H.264 would end up being YUV 4:4:4
   may not be playable with all players, set pixel format to YUV 4:2:0 explicitely
	''$ ffmpeg -i '%03d.png' -c:v libx264 -pix_fmt yuv420p out.mp4''
* some players might require "''-vf format=yuv420p''" or "''-pix_fmt yuv420p''"
* if problems with frames occur, use "''fps''" filter
	''$ ffmpeg -framerate 1/5 -i %03d.png -c:v libx264 -vf fps=25 -pix_fmt yuv420p out.mp4''
* ''$ ffmpeg -f image2 -i %03d.jpg out.mpg''
* **create a video that has only one image** with a specific time duration (title or credits for example)
	''$ ffmpeg -loop 1 -framerate 25 -i title_blurred.png -c:v libx264 -t 5 -pix_fmt yuv420p -vf scale=1440:-2 title.mp4''
		''-loop 1''		loop over the input if set to 1
		''-t 5''		set length to 5 seconds
		
* **get the framerate** of a video
	''$ ffmpeg -i video.mp4''
		look for the ''tbr'' value

* **extract frames** from a video
	* write a single frame into image
		''$ ffmpeg -i input.mov -ss 00:00:14.435 -vframes 1 out.png''
	* write image every second 
		''$ ffmpeg -i input.mov -vf fps=1 out%03d.png''
	* write image every minute
		''$ ffmpeg -i input.mov -vf fps=1/60 out%03d.jpg''
	* write image every frame (assuming framerate is 24)
		''$ ffmpeg -i input.mov -r 24/1 out%03d.jpg''

* **add still image before first frame** of video
	'''
	$ ffmpeg -loop 1 -framerate FPS -t SECONDS -i IMAGE \
	       -t SECONDS -f lavfi -i aevalsrc=0 \
	       -i INPUTVIDEO \
	       -filter_complex '[0:0] [1:0] [2:0] [2:1] concat=n=2:v=1:a=1' \
	       [OPTIONS] OUTPUT
			''-loop 1 -framerate FPS -t SECONDS -i IMAGE	open image, loop over it making video with SECONDS and FPS framerate (must be same as video!)''
			''-t SECONDS -f lavfi -i aevalsrc=0''		generate silence for SECONDS
			''-filter_complex '[0:0] [1:0] [2:0] [2:1] concat=n=2:v=1:a=1'''
																open file 0 stream 0 (image), file 1 stream 0 (silence audio), file 2 stream 0 and 1 (input audio and video)
																use concat to concatenate them, n=number of segments, v=number of output videos, a=number of output audio
		Example:
			''$ ffmpeg -loop 1 -framerate 25 -t 7 \''
				''-i blackboards/FusionResearch_titlepage.png -t 7 -f lavfi \''
				''-i aevalsrc=0 -i FR10_concat.mp4 \''
				''-filter_complex '[0:0] [1:0] [2:0] [2:1] concat=n=2:v=1:a=1' \''
				''-codec:v libx264 -crf 18 -codec:a aac -b:a 192k FR10_concat_titlepage.mp4''
	'''

* **add fade out** (light to dark) at the end or fade in (dark to light) at beginning
	''$ ffmpeg -i start.mp4 -y -vf fade=in:0:25 start_fadein.mp4''
	''$ ffmpeg -i end.mp4 -y -vf fade=out:100:25 end_fadeout.mp4''
		''-y''		overwrite out file without asking
		''-vf''		short (alias) for ''-filter:v''
* add **fade** in and out of **video and sound** 
	'''
	$ ffmpeg -i video_in.mp4 -filter_complex \
		"[0:v]fade=type=in:start_time=0:duration=1,
	              fade=type=out:start_time=3:duration=1[v];
	         [0:a]afade=type=in:start_time=0:duration=2,
	              afade=type=out:start_time=30.57:duration=2[a]" \
	        -map "[v]" -map "[a]" video_out.mp4
	'''

		
* make video **play faster** (assuming there is no audio)
	''$ ffmpeg -i input.mp4 -filter:v "setpts=PTS/4" output_4x.mp4''

* **concatenate videos** with same codecs
	* create a file ''mylist.txt'' with all files to be concatenated:

		''# file listing all files to be concatenated''
		''file '/path/to/file1.mp4'''
		''file '/path/to/file1.mp4'''
		
		alternatively, and if all files in current directory are to be used, a single command can do this
		''$ printf "file '%s'\n" * > myclips.txt''
	* concatenate with ffmpeg
		''$ ffmpeg -f concat -safe 0 -i mylist.txt -c copy output.mp4''
			''-safe 0''		not required if paths are relative
	* concatenate with ffmpeg, is previously video has been cut with ffmpeg which might result in broken timetamps
		''$ ffmpeg -safe 0 -f concat -segment_time_metadata 1 -i mylist.txt -vf select=concatdec_select -af aselect=concatdec_select,aresample=async=1 output.mp4''
	* if first video has no audio, dummy audio track must be added first, otherwise final video will have no audio (as setup of first video is used for final video)
		''$ ffmpeg -i input.mp4 -f lavfi -i anullsrc=cl=stereo:r=48000 -shortest -c:v copy output.mp4''
			''-i anullsrc=cl=stereo:r=48000''	make sure that these settings match settings of other audio tracks
	* lossless concatenation of mp4 files
		''$ ffmpeg -i video1.mp4 -c copy -bsf:v h264_mp4toannexb -f mpegts video1.ts''
		''$ ffmpeg -i video2.mp4 -c copy -bsf:v h264_mp4toannexb -f mpegts video2.ts''
		''$ ffmpeg -i "concat:video1.ts|video2.ts" -c copy -bsf:a aac_adtstoasc output.mp4''

* **concatenate with crossfade** between the videos
{{{code: lang="sh" linenumbers="True"
#!/usr/bin/env bash

# inspired by https://superuser.com/questions/1001039/what-is-an-efficient-way-to-do-a-video-crossfade-with-ffmpeg

# set filenames of videos to concatenate
vid0_fname="0.mp4"
vid1_fname="1.mp4"
vid2_fname="2.mp4"
vid3_fname="3.mp4"
vid4_fname="4.mp4"

# get exact length of all videos
vid0_length=`ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 ${vid0_fname}`
vid1_length=`ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 ${vid1_fname}`
vid2_length=`ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 ${vid2_fname}`
vid3_length=`ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 ${vid3_fname}`
vid4_length=`ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 ${vid4_fname}`

# set crossfade times
fadeout_time=1
fadein_time=1

# set start time of fadeout for each video
vid0_fadeout=$(bc -l <<<"${vid0_length}-${fadeout_time}")
vid1_fadeout=$(bc -l <<<"${vid1_length}-${fadeout_time}")
vid2_fadeout=$(bc -l <<<"${vid2_length}-${fadeout_time}")
vid3_fadeout=$(bc -l <<<"${vid3_length}-${fadeout_time}")
vid4_fadeout=$(bc -l <<<"${vid4_length}-${fadeout_time}")

echo $vid0_fname $vid0_length $vid0_fadeout

# this is where the magic happens
ffmpeg -i $vid0_fname -i $vid1_fname -i $vid2_fname -i $vid3_fname -i $vid4_fname -an \
    -filter_complex "\
        [0:v]trim=start=0:end=${vid0_fadeout},setpts=PTS-STARTPTS[clip0]; \
        [1:v]trim=start=${fadein_time}:end=${vid1_fadeout},setpts=PTS-STARTPTS[clip1]; \
        [2:v]trim=start=${fadein_time}:end=${vid2_fadeout},setpts=PTS-STARTPTS[clip2]; \
        [3:v]trim=start=${fadein_time}:end=${vid3_fadeout},setpts=PTS-STARTPTS[clip3]; \
        [4:v]trim=start=${fadein_time},setpts=PTS-STARTPTS[clip4]; \
        [0:v]trim=start=${vid0_fadeout}:end=${vid0_length},setpts=PTS-STARTPTS[fadeoutclip0]; \
        [1:v]trim=start=${vid1_fadeout}:end=${vid1_length},setpts=PTS-STARTPTS[fadeoutclip1]; \
        [2:v]trim=start=${vid2_fadeout}:end=${vid2_length},setpts=PTS-STARTPTS[fadeoutclip2]; \
        [3:v]trim=start=${vid3_fadeout}:end=${vid3_length},setpts=PTS-STARTPTS[fadeoutclip3]; \
        [1:v]trim=start=0:end=${fadein_time},setpts=PTS-STARTPTS[fadeinclip1]; \
        [2:v]trim=start=0:end=${fadein_time},setpts=PTS-STARTPTS[fadeinclip2]; \
        [3:v]trim=start=0:end=${fadein_time},setpts=PTS-STARTPTS[fadeinclip3]; \
        [4:v]trim=start=0:end=${fadein_time},setpts=PTS-STARTPTS[fadeinclip4]; \
        [fadeinclip1]format=pix_fmts=yuva420p, fade=t=in:st=0:d=1:alpha=1[fadein1]; \
        [fadeinclip2]format=pix_fmts=yuva420p, fade=t=in:st=0:d=1:alpha=1[fadein2]; \
        [fadeinclip3]format=pix_fmts=yuva420p, fade=t=in:st=0:d=1:alpha=1[fadein3]; \
        [fadeinclip4]format=pix_fmts=yuva420p, fade=t=in:st=0:d=1:alpha=1[fadein4]; \
        [fadeoutclip0]format=pix_fmts=yuva420p, fade=t=out:st=0:d=1:alpha=1[fadeout0]; \
        [fadeoutclip1]format=pix_fmts=yuva420p, fade=t=out:st=0:d=1:alpha=1[fadeout1]; \
        [fadeoutclip2]format=pix_fmts=yuva420p, fade=t=out:st=0:d=1:alpha=1[fadeout2]; \
        [fadeoutclip3]format=pix_fmts=yuva420p, fade=t=out:st=0:d=1:alpha=1[fadeout3]; \
        [fadein1]fifo[fadeinfifo1]; \
        [fadein2]fifo[fadeinfifo2]; \
        [fadein3]fifo[fadeinfifo3]; \
        [fadein4]fifo[fadeinfifo4]; \
        [fadeout0]fifo[fadeoutfifo0]; \
        [fadeout1]fifo[fadeoutfifo1]; \
        [fadeout2]fifo[fadeoutfifo2]; \
        [fadeout3]fifo[fadeoutfifo3]; \
        [fadeoutfifo0][fadeinfifo1]overlay[crossfade1]; \
        [fadeoutfifo1][fadeinfifo2]overlay[crossfade2]; \
        [fadeoutfifo2][fadeinfifo3]overlay[crossfade3]; \
        [fadeoutfifo3][fadeinfifo4]overlay[crossfade4]; \
        [clip0][crossfade1][clip1][crossfade2][clip2][crossfade3][clip3][crossfade4][clip4]concat=n=9[output]; \
        [0:a]volume=1.0[vid0_audio]; \
        [1:a]volume=1.0[vid1_audio]; \
        [2:a]volume=1.75[vid2_audio]; \
        [3:a]volume=1.0[vid3_audio]; \
        [4:a]volume=1.0[vid4_audio]; \
        [vid0_audio][vid1_audio]acrossfade=d=1[audio1]; \
        [audio1][vid2_audio]acrossfade=duration=1[audio2]; \
        [audio2][vid3_audio]acrossfade=duration=1[audio3]; \
        [audio3][vid4_audio]acrossfade=duration=1[audio4] \
        " \
    -map "[output]" -codec:v libx264 -crf 18 \
    -map "[audio4]" -codec:a aac -b:a 192k \
    out_volAdjusted.mp4 
}}}

'''
	''trim''				break each input stream (each video) into two parts: content + cross fade section (fade out: content + fade section; fade in: fade section + content)
	''setpts=PTS-STARTPTS''		this filter makes each subclip start at 0 seconds 
	''fadeinclip1''			specify fade in clip
		''format=pix_fmts=yuva420p	''any format which handles transparency is ok
		''alpha=1''				video itself will not darken, transparency amout will fade
		''st''				start
		''d''				duration
	''fifo''				this filter ensures that buffer space is available in filter complex (if not set, crossfade might fail)
	''overlay''				overlay the fade-out with following fade-in section (ensuring earlier that the two crossfade sections have same size, we don't need to take care of complicated overlay filter options)
	''concat=n=9''			line-up all segments and concatenating them to a video
'''

* **re-scale a video**
	''$ ffmpeg -i input.mp4 -s 720x480 -c:a copy output.mp4''
* **remove audio from video**
	''$ ffmpeg -i input.mp4 -c copy -an nosound.mp4''
	
* **cut a video**
	''$ ffmpeg -ss 00:03:46 -i input.mp4 -to 00:04:48 -c copy output.mp4''
	''$ ffmpeg -ss 00:00:30.0 -i input.mp4 -c copy -t 00:00:10.0 output.mp4''
	''$ ffmpeg -ss 00:00:03.0 -i input.mp4 -codec:v libx264 -crf 18 -codec:a aac -b:a 192k -t 00:19:47 output.mp4''
		this enforces a full re-encoding which takes quite some time, but might be the only option to get precise cuts (due to keyframe misalignment issues)
		bitrate is set to such a high value because with the default value (128k), the quality decreased significantly 
		
* **crop a video**
	''$ ffmpeg -i input.mp4 -filter:v "crop=out_w:out_h:x:y" output.mp4''
		''out_w''		width of output rectangle
		''out_h''		height of output rectangle
		''x'', ''y''		top left corner of output rectangle (coordinate system 0 is top left)
* get **video information**
	''$ ffprobe video.mp4''
* **add audio to video**
	''$ ffmpeg -i input.mp4 -i input.mp3 -c copy -map 0:v:0 -map 1:a:0 output.mp4''
		''0:v:0''			first video stream of first file
		''1:a:0''			first audio stream of second file
		''-c copy''			copies audio and video stream (fast)
		''-shortest	''	have ffmpeg stop conversion when the shorter of the two input ends
* **add audio from one video to another video**
	$ ''ffmpeg -an -i video_main.mp4 -vn -i video_sound2add.mp4 \''
		''-map 0:v -map 1:a -c:v copy -c:a copy output.mp4''
		''-an -i $VIDEO1''		ignore audio stream
		''-vn -i $VIDEO2''		ignore video stream
* **extract audio** from video (not sure which one is better)
	extract audio without re-encoding
	''$ ffmpeg -i video.mp4 -vn -acodec copy only_sound.mp3''
	better practice is the following
	''$ ffmpeg -i video.mp4 -q:a 0 -map a only_audio.mp3''
* **reduce volume**
	''$ ffmpeg -i music_in.mp3 -filter:a "volume=0.5" music_out.mp3''

* **add text to video**
	''$ ffmpeg -i video.mp4 -vf \''
		''"[in]drawtext=''
			''fontfile=OpenSans-Regular.ttf:''
			''fontsize=40:''
			''fontcolor=white:''
			''x=(w-text_w)/2:''
			''y=h-60*t:''
			''textfile='my_text.txt'[out]" \''
		''-c:v libx264 -t 10 video_out.mp4''
		
'''
		''-vf''				short for video filters, syntax for filters is always
			''-vf "filter1=''
				''setting1=value1:''
				''setting2=value2, ''
			   ''filter2=''
				''setting1=value1:''
				''setting2:value2"''
		'':enable='between(t,8,15)'''	text will appear at 8 seconds and disappear at 15 seconds
		''drawtext=fontfile=<font>	''available fonts can be listed with  ''$ fc-list''
'''
		
'''
	''$ ffmpeg -i video.mp4 \''
		''-vf  "[in]drawtext=''
			''fontfile=<path_for_ttf_file>:''
			''fontsize=40:''
			''fontcolor=white:''
			''x=(w-text_w)/2:''
			''y=h-60*t:''
			''textfile='my_text.txt':''
			''enable='between(t,1,4)', ''
		''drawtext=''
			''fontfile=OpenSans-Regular.ttf:''
			''fontsize=40:''
			''fontcolor=white:''
			''x=w*.65:''
			''y=h*.6:''
			''text='sorry, buffer xrun':''
			''enable='between(t,454.5,456.5)'''''[out]" \''
		''-c:v libx264 -t 10 video_out.mp4''
'''
	
* **filter_complex** example from ffmpeg documentation
	''$ ffmpeg -f lavfi -i testsrc -f lavfi -i testsrc -f lavfi -i testsrc -f lavfi -i testsrc -filter_complex \''
		''"[1:v]negate[a]; \''
		 ''[2:v]hflip[b]; \''
		 ''[3:v]edgedetect[c]; \''
		 ''[0:v][a]hstack=inputs=2[top]; \''
		 ''[b][c]hstack=inputs=2[bottom]; \''
		 ''[top][bottom]vstack=inputs=2[out]" -map "[out]" -c:v ffv1 -t 5 multiple_input_grid.avi''

'''
	''-f lavfi -i testsrc			''declaration of input, input format is lavfi, input file is testsrc; duplicated four times to initiate four different inputs
	''-filter_complex''				
		''[1:v]negate[a]''				take video from input at index 1, use negate filter on it and 'save' output of the filter to a link named a
		''[2:v]hflip[b]''				take video from input at index 2, use hflip filter on it and 'save' output of the filter to a link named b
		''[3:v]edgedetect[c]''			take video from input at index 3, use edgedetect filter on it and 'save' output of the filter to a link named c
		''[0:v][a]hstack=inputs=2[top]''		take two inputs: video from input at index 0 + something that is in the link a (which is output of [1:v]negate filtering)
													 then perform on these two inputs filter hstack with one parameter ''inputs'' that is set to 2; save result in link named top
		''[b][c]hstack=inputs=2[bottom]''		similar to above: take two inputs from links b and c, perform filter hstack with parameter inputs set to 2, save result in link named bottom
		''[top][bottom]vstack=inputs=2[out]	''take two inputs from links top and bottom, perform vstack filter with param inputs set to 2 and save the result in link named out
	''-map''					map stuff from link named out as input to next arguments
	''-c:v ffv1				''use FFV1 codec for video
	''-t 5''					result should be a 5 second video
	''multiple_input_grid.avi''			name of the output video
'''
	
* **overlay image for a certain time** span
	'''
	$ ffmpeg -i input.mp4 -i watermark.png \
		-filter_complex \
		"[0:v][1:v] overlay=
			x=10:
			y=10:
			enable='between(t,1,2)'" \
		output.mp4
	'''

* **overlay multiple images** at different times
	''$ ffmpeg -i video_in.mp4 \''
		''-i image1.png -i image2.jpeg -i image3.png \''
		''-filter_complex \''
			''"[0][1]overlay=''
				''x=10:y=200:enable='between(t,655,665)'[v1]; ''
			''[v1][2]overlay=x=10:y=300:enable='between(t,1189,1190)'[v2]; ''
			''[v2][3]overlay=x=400:y=300:enable='between(t,814,820)'[v3]" \''
		''-map "[v3]" -map 0:a -c:a libx264 -crf 18 -c:a copy video_out.mp4''

* more **filter_complex** examples
	''$ ffmpeg -i myvideo.mp4 -i image.png \''
		''-filter_complex \''
		''[0:v][1:v]overlay=''
			x=''5:''
			y=''5,''
		''drawtext=text=mytext:''
			''fontcolor=orange@1.0:''
			''fontsize=30:''
			''x=30:''
			''y=200[v] \''
		''-map [v] -map 0:a -c:a copy output.mp4''
		
* **filter_complex** example to **move overlay to specific position** 
	'''
	$ ffmpeg -i input.mp4 -i image.png -y \
	       -filter_complex "[0:v][1:v]overlay=
			x='if(eq(n,439),300,0)':
			y='if(eq(n,439),300,0)':
			enable='eq(n,438)+eq(n,439)'[out]" \
	       -map [out] -map 0:a -ss 17 output.mp4
	'''

* **add a logo and simulateously text** to a video with **filter_complex**
	''$ ffmpeg -i video.mp4 -i logo.png \''
		''-filter_complex "[0:v][1:v]overlay=10:10,''
			''drawtext=text='Hello World'" \''
		''-c:a copy -movflags +faststart output.mp4''
		
* **add animated text** to a video 
	''$ ffmpeg -i input -vf \''
		''"drawtext=enable='gte(t,3)':''
		''fontfile=Vera.ttf:''
		''textfile=text.txt:''
		''reload=1:''
		''y=h-line_h-10:''
		''x=(W/tw)*n" \''
	''output''
	==> enable ''drawtext'' filter after 3 seconds
		every time ''text.txt'' is updated, text appearance will change
		text moves on screen from left to right
		reload=1		textfile will be reloaded before every frame
		
* **fade in/out text** in a video
	* get the command from here: http://ffmpeg.shanewhite.co/
	* example command gotten from that webpage: fade-in start at t=0, fade-in duration t=1, time for text to be shown t=10, fade-out duration t=1
	''$ ffmpeg -i in.mp4 -filter_complex \''
		''"[0:v]drawtext=fontfile=Sans-Bold.ttf:''
		''text='some text':''
		''fontsize=40:''
		''fontcolor=ffffff:''
		''alpha='if(lt(t,0),0,if(lt(t,1),(t-0)/1,if(lt(t,11),1,if(lt(t,12),(1-(t-11))/1,0))))':''
		''x=(w-text_w)/2:''
		''y=h*.03" \''
		''out.mp4''
	-> ''lt(var,value)'' is //less than// function
		
* **add animated GIF to a video** (known as //picture in picture//)
	''$ ffmpeg -i video_in.mp4 -ignore_loop 0 -i animated_gif.gif \''
		''-filter_complex "[1:v]scale=240:135[ovrl];''
			''[0:v][ovrl]overlay=0:590:enable='between(t,180.5,182)':shortest=1" \''
			''-codec:a copy -codec:v libx264 -crf 18 video_out.mp4''
			
* add **video in a video** (also known as picture-in-picture)
	''$ ffmpeg -i main_video.mp4 -i short_overlay_video.mp4 \''
		''-filter_complex "[1]setpts=PTS-STARTPTS+560/TB[ovrl];''
			''[0:v][ovrl]overlay=x=400:y=460:enable='between(t,560,566)'" \''
		''-codec:a copy -codec:v libx264 -crf 18 output.mp4''
	==> import to set overlay video to start play when it is supposed to appear (see the ''setpts'' option),
		otherwise, the overlay video starts to play when main video starts
		
* get **mouse position** (might help for getting coordinates) or **window information**
	''$ xdotool getmouselocation'' 
	''$ xwininfo -frame'' 


===== Video capture with ubuntu =====
* **screen capture** (including sound) with **recordMyDesktop**
	1. install the software
		''sudo apt-get install recordMyDesktop		'' command-prompt version//
		''sudo apt-get install gtk-recordMyDesktop'' 	 GUI version//
	2. record
		''recordmydesktop -x START_POS -y START_POS --width XSIZE --height YSIZE -o foo.ogv''
				//// e.g. recordmydesktop -x 10 -y 10 --width 640 --height 480 -o foo.ogv//
		''wininfo						'' get size and position of window//
	2a. record with the gui (set windowsize and sound)
* **video capture** using **ffmpeg** and the build-in webcam
	1. using ffmpeg w/o sound
		''ffmpeg -f video4linux2 -s 640x480 -i /dev/video0 -qscale 3 out.mpeg''
	2. using ffmpeg w/ sound
		''ffmpeg -t 00:00:15 -f video4linux2 -s 640x480 -i /dev/video0 -f alsa -ac 2 -i pulse -qscale 3 outSound.mpeg''
* **get info about webcam** and supported resolutions and framerates
	''$ ffmpeg -f video4linux2 -list_formats all -i /dev/video0''
	''$ v4l2-ctl --list-formats-ext''
* **screen capture** using **ffmpeg**
	a. ''ffmpeg -f x11grab -r 25 -s 320x240 -i :0.0+10,20 out.mpg''
		''-s 320x240''		set frame size of recorded video to 320x240, shortcut for ''-video_size''
		''-r 25''			set framerate to 25, shortcut for ''-framerate''
		''-i: 0.0+10,20''		starting upper left corner of frame to record at ''(10,20)'', ''0.0'' is ''display_number.screen_number'' of X11 server, same as DISPLAY environment variable
								   one could also write:   ''-i: "${DISPLAY}.0"''
	b. ''ffmpeg -f x11grab -r 25 -s 640x480 -i :0.0 -vcodec huffyuv ffmepg_screencast.mkv''
									record with 25 framerate a window of 640x480 and with huffyuv codec
										note: will produce a HUGE file
		''ffmpeg -i in.mkv -vcodec libx264 -acodec aac -crf 22 -pix_fmt yuv420p -threads 0 out.mp4''
									--> compress to mp4 format (youtube-ready)
			''-crf''			Contant Rate Factor, for x264 range is 0-51 (depends on codec), where 0 is lossless, 23 is default, 51 is worst possible
									   lower value means higher quality
									   18 is often considered to be visually lossless
			''-threads''		setting to configure number of threads to be used, default value is 0, which tries to optimize thread usage using all available cores
			''-pix_fmt yuv420p	''default value is ''yuv444p'' which isn't compatible with all browsers (i.e. video would not be able to play in e.g. firefox)
									   if color quality seems to be somewhat reduced, try ''yuvj420p''
			  note: -vpre could be set to ensure using same presets
	c. ''ffmpeg -s 1024x768 -r 25 -f x11grab -i :0.0+100,200 output.flv''
									record 1024x768 starting with upper left corner at (100,200)
	d. ''ffmpeg -s 1024x768 -r 25 -f x11grab -i :0.0+100,200 -f alsa -ac 2 -i pulse output.flv''
									same as b. but **with sound**
			''-f alsa <input_option> -i <input_device>''
									capturing audio with ffmpeg and ALSA, <input_device> tells ffmpeg which audio capturing card or device to use
									NOTE: rebooting might change order of sound devices
									''alsamixer'' is a helpful tool to set/control audio devices
			''-f alsa -ar xxx''		audio sample rate, default value is usually "44100" (Hz)
			''-f alsa -ac x''		audio channels, default value is "1" (mono) for Mic input
	e. ''arecord -l''
									to identify correct microphone (in case of **external mic**)
		''ffmpeg -f x11grab -r 25 -s 1280x720 -i :0.0+320,236 -f alsa -i hw:1 -vcodec huffyuv lect_video.mkv''
									to use "card 1" for audio input
	f. ''ffmpeg -f x11grab -thread_queue_size 1024 -r 25 -s 1280x720 -i :0.0+320,236 -f alsa -thread_queue_size 1024 -i hw:1 -vcodec huffyuv screencast_video.mkv''
			''-thread_queue_size''	sets maximum number of queued packets when reading from the file or device. 
									   With low latency / high rate live streams, packets may be discarded if they are not read in a timely manner; raising this value can a
									void it.
									   (otherwise, ''ALSA buffer xrun'' error appeared from time to time, leading to loss of audio packets)
	g. ''ffmpeg -video_size 1920x1080 -framerate 30 -f x11grab -i :0.0 -c:v libx264rgb -crf 0 -preset ultrafast output.mkv''
			''-crf 0''			tells x264 to encode in lossless mode (to reduce hardware requirements, **if CPU is not fast** enough)
			''-preset ultrafast''	do encoding fast
			''-c:v libx264rgb''		using instead ''libx264'' would be a lossy conversion from RGB to yuv444p	
	h. ''ffmpeg -f alsa -ac 2 -i hw:0,0 -f x11grab -r 30 \''
		''-s $(xwininfo -root | grep 'geometry' | awk '{print $2;}') \''
		''-i :0.0 -acodec pcm_s16le -vcodec libx264 -preset ultrafast -crf 0 -threads 0 -y screencast.avi''
			''xwininfo''		return size of window user clicks into (to define ''video_size'')
			''-y''			overwrite output file without asking
	i. ''ffmpeg -f alsa -ac 2 -i hw:0,0 -f x11grab -r 30 \''
		''-s $(xwininfo -frame | grep -oEe 'geometry [0-9]+x[0-9]+' | grep -oEe '[0-9]+x[0-9]+') \''
		''-i :0.0+$(xwininfo -frame | grep -oEe 'Corners:\s+\+[0-9]+\+[0-9]+' | grep -oEe '[0-9]+\+[0-9]+' | sed -e 's/\+/,/' ) \''
		''-acodec pcm_s16le -vcodec libx264 -preset ultrafast -crf 0 -threads 0 screencast.avi''
									record a single window, clicking mouse on window to start recording
	j. putting everything into a bash script
{{{code: lang="sh" linenumbers="True"
#!/bin/sh -

# Example values
pulseaudio_source=0
thread_queue_size=32
video_size=1920x1080
framerate=25
record_area=$DISPLAY+0,0
naudio_channels=2
outfile=capture.mkv

# Command
ffmpeg -thread_queue_size $thread_queue_size -video_size $video_size \
    -framerate $framerate -f x11grab -i $record_area \
    -thread_queue_size $thread_queue_size -f pulse -ac $naudio_channels \
    -channel_layout stereo -i $pulseaudio_source -c:v libx264 -qp 0 \
    -preset ultrafast -c:a libvorbis $outfile
}}}
	
* **screen capture** and **desktop audio capture** with ffmpeg 
	1. start recording with ffmpeg, for example
		''$ ffmpeg -f x11grab -thread_queue_size 4096 -r 25 -s 1920x1080 -i :0.0+0+0 \''
			''-f pulse -thread_queue_size 4096 -i default \''
			''-vcodec libx264 -pix_fmt yuv420p -threads 0 -acodec aac video.mp4''
	2. start ''pavucontrol'' 
		''$ pavucontrol''
	3. go to the ''Recording'' tab, you will find ''ffmpeg'' or ''Lavf57.83.100'' or similar there
	4. change audio capture from  //Built-in Audio Analog Stereo//  to  //Monitor of Built-in Audio Analog Stereo//
	5. NOTE: this setting will be remember when you start recording the next time

'''
	Alternatively, one can enter the internal name of the monitor source manually:
		''$ pactl list sources'' 			get name of monitor source
		''$'' ''ffmpeg -f x11grab -thread_queue_size 4096 -r 25 -s 1920x1080 -i :0.0+0+0 \''
					''-f pulse -thread_queue_size 4096 -i alsa_output.pci-0000_00_1f.3.analog-stereo.monitor \''
					''-vcodec libx264 -pix_fmt yuv420p -threads 0 -acodec aac elearning_3.mp4''
'''

'''
	One can make it even more convenient (source: https://unix.stackexchange.com/a/489522 )
'''
{{{code: lang="sh" linenumbers="True"
A="$(pacmd list-sources | grep -PB 1 "analog.*monitor>" | head -n 1 | perl -pe 's/.* //g')"
F="$(date --iso-8601=minutes | perl -pe 's/[^0-9]+//g').mkv"
V="$(xdpyinfo | grep dimensions | perl -pe 's/.* ([0-9]+x[0-9]+) .*/$1/g')"
ffmpeg -video_size "$V" -f x11grab -i :0.0 -f pulse -i "$A" -filter_complex amerge -ac 2 -preset veryfast "$F"

# note: for a second audio source, e.g. a microphone, one simply needs to add another '-f pulse -i default'
#       before the filter_complex (assuming that the microphone is the default audio input source) 
}}}
	
 
* some weblinks:
	* https://web2.ph.utexas.edu/~asimha/PHY303K_Spring2015/making.htm
	* https://sandilands.info/sgordon/presenting-and-recording-lectures-with-ubuntu-linux


===== Live video from webcam =====
* using **vlc**
		''$ cvlc v4l2:///dev/video0''
		''$ cvlc v4l2:///dev/video1:width=640:height=480''
* using **ffmpeg**
		? (stream to server)
		''$ ffplay /dev/video0''
* using **cheese**
		''$ sudo apt-get install cheese''
* see also
	  https://help.ubuntu.com/community/Webcam
	  http://www.netinstructions.com/tag/logitech-quickcam-pro-9000/
	  http://xmodulo.com/live-stream-video-webcam-linux.html
* **get resolution** of webcam via command line
	a. locate webcam
		$ lsusb
		   output might look like this
		   ...
		   ''Bus 003 Device 003: ID 046d:0991 Logitech, Inc. QuickCam Pro for Notebooks''
		   ...
	b. greb possible Width and Height values
		''$ lsusb -s 003:003 -v | egrep "Width|Height"''
		
		

===== Remove background of live video (green screen, chroma keying) =====
1. A few notes about green screens
	* green screen hex code is #00b140; RGB is (0,177,64); CMYK is (81,0,92,0), websafe color is #009933
		* #34D454 (0x34D454) might also work, is in RGB (52,212,84)
	* green is most popular chroma color because camera sensors can see it more accurately than any other color
	* green color channel produces less noise than others
	* green is less commonly worn in wardrobes
	* green is further from skin tones and red or blue
	* chroma key colors does not have to be green
	* //digital green// (in contrast to //chroma key green//) is a brighter neon green color, developed to further differentiate foreground and background, popular in streaming nowadays
	* Chroma Key is compositing technique with green screen to superimpose two images or clips into a new image or clip
2. Use **OBS** (Open Broadcaster Software)
	a. install it: https://obsproject.com/
	b. install virtual webcam plugin for linux (for Windows, there is a different one): https://github.com/CatxFish/obs-v4l2sink/releases/
	c. install video4linux2: ''$ ''
''sudo apt-get install v4l2loopback-dkms''
	d. load video4linux2 into kernel (might require reboot): ''$ sudo modprobe v4l2loopback''
	e. start OBS
	f. Sources → + button → Video Capture Device (V4L2)
	g. add your webcam as input source
	h. give it a proper name
	i. back in main window, right-click your new webcam source
	j. select Filters
	k. Effect Filters → + button → Chroma Key
	l. choose appropriate color (green for green screen), adjust the //similarity// parameter
	m. back in main window, add another source (can be done via drag and drop from explorer)
	n. choose an image which will be your virtual background
	o. alternatively, choose a video, right click, Properties, tick Loop
	p. Tools → v4l2sink
	q. Choose as device path the virtual webcam (loaded/generated by v4l2loopback, check which one it is), click Start
4. Use **ffmpeg** and a **video4linux2** (v4l2) pseudo device using **v4l2loopback**
	a. install required software 
		''$ sudo apt-get install v4l2loopback-dkms'' 
	b. reboot
	c. load (enable) video4linux2 loopback device in kernel, creating a pseudo video device, usually ''/dev/video1''
		''$ sudo modprobe v4l2loopback'' 
	d. start capturing onto the pseudo video device, which can be later used as video source
		''$ ffmpeg -i background.jpg -f v4l2 -pix_fmt mjpeg \''
			''-framerate 30 -video_size 1280x720 -i /dev/video0 \''
			''-filter_complex "[0:v]scale=1280x720[bg];[1:v]colorkey=0x00b140:0.5:0.0[ckout];[bg][ckout]overlay" -pix_fmt yuv420p -f v4l2 /dev/video1''
			
'''
			''colorkey=<color>:<similarity>:<blend>''		color is rgb color to match in hex (e.g. 0x00b140), to pick a color from an image, use e.g. https://html-color-codes.info/colors-from-image/
																similariy (percentage) is tolerance on color match, 0.01 must match exactly, 1.0 matches everything
																blend percentage, 0.0 makes pixels either fully transparent or not, 
																	higher values results in semi-transparent pixels (higher transparency, when more similar)
	e. any video software can now simply access the pseudo video device
		''$ ffplay /dev/video0''
	f. Instead of replacing background with a static image, one can also **loop a video in the background**
		''$ ffmpeg -stream_loop 5 -re -i bg_video.mp4 -f v4l2 -pix_fmt mjpeg \''
			''-framerate 30 -video_size 1280x720 -i /dev/video0 \''
			''-filter_complex "[0:v]scale=1280x720[bg]; \''
				''[1:v]colorkey=0x61A051:0.15:0.0[ckout]; \''
				''[bg][ckout]overlay" \''
			''-pix_fmt yuv420p -f v4l2 /dev/video1''
'''

5. Use latest version of **Webcamoid** from github repository, https://github.com/webcamoid/webcamoid
6. Use a **neural network** to recognize person in foreground and background and **v4l2loopback** to create virtual webcam
	a. https://github.com/allo-/virtual_webcam_background and https://www.virtual-webcam.com/
7. Use Python, **OpenCV**, Tensorflow, and Node.Js
	a. https://github.com/fangfufu/Linux-Fake-Background-Webcam
8. **Subtract background** with **Python** and **OpenCV** 
	a. full code example: https://gist.github.com/drscotthawley/2d6bbffce9dda5f3057b4879c3bd4422
	b. general explanation: http://grauonline.de/wordpress/?page_id=3065


===== Merge video files =====
* merge **avi-files**
   install required apps
	''$ sudo apt-get install transcode transcode-utils''
   merge the avi-files
	''$ avimerge -i part1.avi part2.avi -o joinedfile.avi''
* merge **mkv-files**
   install mkvtoolnix
	''$ sudo apt-get install mkvtoolnix mkvtoolnix-gui''
   Start mmg (graphical user interface for mkvmerge)
	   "Add" the first file, "Append" the second one
	   "Append" third, fourth, ..., Set output name in the textbox at the bottom
	   "Start muxing"


===== Video streaming =====

==== YouTube ====
Access the Live Streaming panel to retrieve the Live Stream URL and the Private Key to be used within the FFmpeg scripts
	1. broadcast, where video and audio are included in a single mp4 file, source: https://github.com/Darkseal/FFmpeg-scripts
{{{code: lang="sh" linenumbers="True"
#! /bin/bash
 
VBR="1500k"
FPS="30"
QUAL="ultrafast"
YOUTUBE_URL=" rtmp://a.rtmp.youtube.com/live2"
YOUTUBE_KEY="XXXXXXXXXX"
VIDEO_SOURCE="sample.mp4"
AUDIO_ENCODER="aac"

$ ffmpeg 
  -re \
  -f lavfi \
  -i "$VIDEO_SOURCE" \
  -c:v libx264 -preset $QUAL -r $FPS -g $(($FPS *2)) -b:v $VBR \
  -c:a $AUDIO_ENCODER -threads 6 -ar 44100 -b:a 128k -bufsize 512k -pix_fmt yuv420p \
  -f $YOUTUBE_URL/$YOUTUBE_KEY 
}}}
		

'''
	2. broadcast, where video and audio are separated and then merged in broadcast, source: https://github.com/Darkseal/FFmpeg-scripts
'''
{{{code: lang="sh" linenumbers="True"
#! /bin/bash
 
VBR="1500k"
FPS="30"
QUAL="ultrafast"
YOUTUBE_URL=" rtmp://a.rtmp.youtube.com/live2"
YOUTUBE_KEY="XXXXXXXXXX"
VIDEO_SOURCE="sample.jpg"
AUDIO_SOURCE="sample.mp3"
AUDIO_ENCODER="aac"
 
ffmpeg \
 -loop 1 \
 -re \
 -framerate $FPS \
 -i "$VIDEO_SOURCE" \
 -thread_queue_size 512 \
 -i "$AUDIO_SOURCE" \
 -c:v libx264 -preset $QUAL -r $FPS -g $(($FPS *2)) -b:v $VBR \
 -c:a $AUDIO_ENCODER -threads 6 -ar 44100 -b:a 128k -bufsize 512k -pix_fmt yuv420p \
 -f flv $YOUTUBE_URL/$YOUTUBE_KEY
}}}
		

'''
	3. broadcast audio only, source: https://github.com/Darkseal/FFmpeg-scripts
'''
{{{code: lang="sh" linenumbers="True"
#! /bin/bash
 
VBR="1500k"
FPS="30"
QUAL="ultrafast"
YOUTUBE_URL=" rtmp://a.rtmp.youtube.com/live2"
YOUTUBE_KEY="XXXXXXXXXX"
VIDEO_SOURCE="sample.jpg"
AUDIO_SOURCE="sample.mp3"
AUDIO_ENCODER="aac"
 
ffmpeg \
 -loop 1 \
 -re \
 -framerate $FPS \
 -i "$VIDEO_SOURCE" \
 -thread_queue_size 512 \
 -i "$AUDIO_SOURCE" \
 -c:v libx264 -tune stillimage -pix_fmt yuv420p -preset $QUAL -r $FPS -g $(($FPS *2)) -b:v $VBR \
 -c:a $AUDIO_ENCODER -threads 6 -ar 44100 -b:a 128k -bufsize 512k -pix_fmt yuv420p \
 -f flv $YOUTUBE_URL/$YOUTUBE_KEY
}}}
		


==== Twitch ====
FFmpeg has a built-in RTMP client (which is the protocol used to send video data to Twitch), FLV (wrapper for audio and video data), H.264 (video codec), and AAC (audio codec).
get RTMP ingest URL: https://stream.twitch.tv/ingests/
	''ffmpeg [input parameters] -vcodec libx264 -b:v 5M -acodec aac -b:a 256k -f flv [RTMP URL/STREAM KEY]''


==== Periscope ====
'''
	1. broadcast, where video and audio are included in a single mp4 file, source: https://github.com/Darkseal/FFmpeg-scripts
'''
{{{code: lang="sh" linenumbers="True"
#! /bin/bash
 
VBR="1500k"
FPS="30"
QUAL="ultrafast"
TWITTER_URL="rtmp://de.pscp.tv:80/x"
TWITTER_KEY="XXXXXXXXXX"
VIDEO_SOURCE="sample.mp4"
AUDIO_ENCODER="aac"
 
$ ffmpeg 
  -re \
  -f lavfi \
  -i "$VIDEO_SOURCE" \
  -c:v libx264 -preset $QUAL -r $FPS -g $(($FPS *2)) -b:v $VBR \
  -c:a $AUDIO_ENCODER -threads 6 -ar 44100 -b:a 128k -bufsize 512k -pix_fmt yuv420p \
  -f $TWITTER_URL/$TWITTER_KEY 
}}}
		

'''
	2. broadcast, where video and audio are separated and then merged in broadcast, source: https://github.com/Darkseal/FFmpeg-scripts
'''
{{{code: lang="sh" linenumbers="True"
#! /bin/bash
 
VBR="1500k"
FPS="30"
QUAL="ultrafast"
TWITTER_URL="rtmp://de.pscp.tv:80/x"
TWITTER_KEY="XXXXXXXXXX"
VIDEO_SOURCE="sample.jpg"
AUDIO_SOURCE="sample.mp3"
AUDIO_ENCODER="aac"
 
ffmpeg \
 -loop 1 \
 -re \
 -framerate $FPS \
 -i "$VIDEO_SOURCE" \
 -thread_queue_size 512 \
 -i "$AUDIO_SOURCE" \
 -c:v libx264 -preset $QUAL -r $FPS -g $(($FPS *2)) -b:v $VBR \
 -c:a $AUDIO_ENCODER -threads 6 -ar 44100 -b:a 128k -bufsize 512k -pix_fmt yuv420p \
 -f flv $TWITTER_URL/$TWITTER_KEY
}}}
		

'''
	3. broadcast audio only, source: https://github.com/Darkseal/FFmpeg-scripts
'''
{{{code: lang="sh" linenumbers="True"
#! /bin/bash
 
VBR="1500k"
FPS="30"
QUAL="ultrafast"
TWITTER_URL="rtmp://de.pscp.tv:80/x"
TWITTER_KEY="XXXXXXXXXX"
VIDEO_SOURCE="sample.jpg"
AUDIO_SOURCE="sample.mp3"
AUDIO_ENCODER="aac"
 
ffmpeg \
 -loop 1 \
 -re \
 -framerate $FPS \
 -i "$VIDEO_SOURCE" \
 -thread_queue_size 512 \
 -i "$AUDIO_SOURCE" \
 -c:v libx264 -tune stillimage -pix_fmt yuv420p -preset $QUAL -r $FPS -g $(($FPS *2)) -b:v $VBR \
 -c:a $AUDIO_ENCODER -threads 6 -ar 44100 -b:a 128k -bufsize 512k -pix_fmt yuv420p \
 -f flv $TWITTER_URL/$TWITTER_KEY
}}}
		

===== References =====
* ffmpeg: https://www.youtube.com/watch?v=M58rc7cxl_s
