Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2019-07-24T05:47:42+02:00

====== Videos ======
Created Mittwoch 24 Juli 2019


===== Create (and modify) videos =====
Using **ffmpeg**
* ''$ ffmpeg -framerate 24 -i %03d.png output.mp4''
	  --> ''-framerate 24''		default is 25 frames per secons
	  --> ''-i '%03d.png'''		specify input files (* does NOT work)
* ''$ ffmpeg -start_number 42 -i '%30d.png' -c:v libx264 out.mp4''
	  --> ''-start_number 42''	default is 0 (can be omitted then)
	  --> ''-c:v libx264''
	  --> ''out.mp4''
* png files use RGB color space, conversion to H.264 would end up being YUV 4:4:4
   may not be playable with all players, set pixel format to YUV 4:2:0 explicitely
	''$ ffmpeg -i '%03d.png' -c:v libx264 -pix_fmt yuv420p out.mp4''
* some players might require "''-vf format=yuv420p''" or "''-pix_fmt yuv420p''"
* if problems with frames occur, use "''fps''" filter
	''$ ffmpeg -framerate 1/5 -i %03d.png -c:v libx264 -vf fps=25 -pix_fmt yuv420p out.mp4''
* ''$ ffmpeg -f image2 -i %03d.jpg out.mpg''
* **create a video that has only one image** with a specific time duration (title or credits for example)
	''$ ffmpeg -loop 1 -framerate 25 -i title_blurred.png -c:v libx264 -t 5 -pix_fmt yuv420p -vf scale=1440:-2 title.mp4''
		''-loop 1''		loop over the input if set to 1
		''-t 5''		set length to 5 seconds
		
* **get the framerate** of a video
	''$ ffmpeg -i video.mp4''
		look for the ''tbr'' value
* **add still image before first frame** of video
	'''
	$ ffmpeg -loop 1 -framerate FPS -t SECONDS -i IMAGE \
	       -t SECONDS -f lavfi -i aevalsrc=0 \
	       -i INPUTVIDEO \
	       -filter_complex '[0:0] [1:0] [2:0] [2:1] concat=n=2:v=1:a=1' \
	       [OPTIONS] OUTPUT
	'''
		''-loop 1 -framerate FPS -t SECONDS -i IMAGE	open image, loop over it making video with SECONDS and FPS framerate (must be same as video!)''
		''-t SECONDS -f lavfi -i aevalsrc=0''		generate silence for SECONDS
		''-filter_complex '[0:0] [1:0] [2:0] [2:1] concat=n=2:v=1:a=1'''
															open file 0 stream 0 (image), file 1 stream 0 (silence audio), file 2 stream 0 and 1 (input audio and video)
															use concat to concatenate them, n=number of segments, v=number of output videos, a=number of output audio
	Example:
		''$ ffmpeg -loop 1 -framerate 25 -t 7 \''
			''-i blackboards/FusionResearch_titlepage.png -t 7 -f lavfi \''
			''-i aevalsrc=0 -i FR10_concat.mp4 \''
			''-filter_complex '[0:0] [1:0] [2:0] [2:1] concat=n=2:v=1:a=1' \''
			''-codec:v libx264 -crf 18 -codec:a aac -b:a 192k FR10_concat_titlepage.mp4''

* **add fade out** (light to dark) at the end or fade in (dark to light) at beginning
	''$ ffmpeg -i start.mp4 -y -vf fade=in:0:25 start_fadein.mp4''
	''$ ffmpeg -i end.mp4 -y -vf fade=out:100:25 end_fadeout.mp4''
		''-y''		overwrite out file without asking
		''-vf''		short (alias) for ''-filter:v'' 
* **concatenate videos** with same codecs
	* create a file ''mylist.txt'' with all files to be concatenated:
		''# file listing all files to be concatenated''
		''file '/path/to/file1.mp4'''
		''file '/path/to/file1.mp4'''
	* concatenate with ffmepg
		''$ ffmpeg -f concat -safe 0 -i mylist.txt -c copy output.mp4''
			''-safe 0''		not required if paths are relative
	* lossless concatenation of mp4 files
		''$ ffmpeg -i video1.mp4 -c copy -bsf:v h264_mp4toannexb -f mpegts video1.ts''
		''$ ffmpeg -i video2.mp4 -c copy -bsf:v h264_mp4toannexb -f mpegts video2.ts''
''$ ffmpeg -i "concat:video1.ts|video2.ts" -c copy -bsf:a aac_adtstoasc output.mp4''

* **re-scale a video**
	''$ ffmpeg -i input.mp4 -s 720x480 -c:a copy output.mp4''
* **remove audio from video**
	''$ ffmpeg -i input.mp4 -c copy -an nosound.mp4''
	
* **cut a video**
	''$ ffmpeg -ss 00:03:46 -i input.mp4 -to 00:04:48 -c copy output.mp4''
	''$ ffmpeg -ss 00:00:30.0 -i input.mp4 -c copy -t 00:00:10.0 output.mp4''
	''$ ffmpeg -ss 00:00:03.0 -i input.mp4 -codec:v libx264 -crf 18 -codec:a aac -b:a 192k -t 00:19:47 output.mp4''
		this enforces a full re-encoding which takes quite some time, but might be the only option to get precise cuts (due to keyframe misalignment issues)
		bitrate is set to such a high value because with the default value (128k), the quality decreased significantly 
		
* **crop a video**
	''$ ffmpeg -i input.mp4 -filter:v "crop=out_w:out_h:x:y" output.mp4''
		''out_w''		width of output rectangle
		''out_h''		height of output rectangle
		''x'', ''y''		top left corner of output rectangle (coordinate system 0 is top left)
* get video information
	''$ ffprobe video.mp4''
* **add audio to video**
	''$ ffmpeg -i input.mp4 -i input.mp3 -c copy -map 0:v:0 -map 1:a:0 output.mp4''
		''0:v:0''			first video stream of first file
		''1:a:0''			first audio stream of second file
		''-c copy''			copies audio and video stream (fast)
		''-shortest	''	have ffmpeg stop conversion when the shorter of the two input ends
* **add text to video**
	''$ ffmpeg -i video.mp4 -vf \''
		''"[in]drawtext=''
			''fontfile=OpenSans-Regular.ttf:''
			''fontsize=40:''
			''fontcolor=white:''
			''x=(w-text_w)/2:''
			''y=h-60*t:''
			''textfile='my_text.txt'[out]" \''
		''-c:v libx264 -t 10 video_out.mp4''
		
		''-vf''				short for video filters, syntax for filters is always
			''-vf "filter1=''
				''setting1=value1:''
				''setting2=value2, ''
			   ''filter2=''
				''setting1=value1:''
				''setting2:value2"''
		'':enable='between(t,8,15)'''	text will appear at 8 seconds and disappear at 15 seconds
		''drawtext=fontfile=<font>	''available fonts can be listed with  ''$ fc-list''
		
	''$ ffmpeg -i video.mp4 \''
		''-vf  "[in]drawtext=''
			''fontfile=<path_for_ttf_file>:''
			''fontsize=40:''
			''fontcolor=white:''
			''x=(w-text_w)/2:''
			''y=h-60*t:''
			''textfile='my_text.txt':''
			''enable='between(t,1,4)', ''
		''drawtext=''
			''fontfile=OpenSans-Regular.ttf:''
			''fontsize=40:''
			''fontcolor=white:''
			''x=w*.65:''
			''y=h*.6:''
			''text='sorry, buffer xrun':''
			''enable='between(t,454.5,456.5)'''''[out]" \''
		''-c:v libx264 -t 10 video_out.mp4''
	
* **filter_complex** example from ffmpeg documentation
	''$ ffmpeg -f lavfi -i testsrc -f lavfi -i testsrc -f lavfi -i testsrc -f lavfi -i testsrc -filter_complex \''
		''"[1:v]negate[a]; \''
		 ''[2:v]hflip[b]; \''
		 ''[3:v]edgedetect[c]; \''
		 ''[0:v][a]hstack=inputs=2[top]; \''
		 ''[b][c]hstack=inputs=2[bottom]; \''
		 ''[top][bottom]vstack=inputs=2[out]" -map "[out]" -c:v ffv1 -t 5 multiple_input_grid.avi''

	''-f lavfi -i testsrc			''declaration of input, input format is lavfi, input file is testsrc; duplicated four times to initiate four different inputs
	''-filter_complex''				
		''[1:v]negate[a]''				take video from input at index 1, use negate filter on it and 'save' output of the filter to a link named a
		''[2:v]hflip[b]''				take video from input at index 2, use hflip filter on it and 'save' output of the filter to a link named b
		''[3:v]edgedetect[c]''			take video from input at index 3, use edgedetect filter on it and 'save' output of the filter to a link named c
		''[0:v][a]hstack=inputs=2[top]''		take two inputs: video from input at index 0 + something that is in the link a (which is output of [1:v]negate filtering)
													 then perform on these two inputs filter hstack with one parameter ''inputs'' that is set to 2; save result in link named top
		''[b][c]hstack=inputs=2[bottom]''		similar to above: take two inputs from links b and c, perform filter hstack with parameter inputs set to 2, save result in link named bottom
		''[top][bottom]vstack=inputs=2[out]	''take two inputs from links top and bottom, perform vstack filter with param inputs set to 2 and save the result in link named out
	''-map''					map stuff from link named out as input to next arguments
	''-c:v ffv1				''use FFV1 codec for video
	''-t 5''					result should be a 5 second video
	''multiple_input_grid.avi''			name of the output video
	
* **overlay image for a certain time** span
	'''
	$ ffmpeg -i input.mp4 -i watermark.png \
		-filter_complex \
		"[0:v][1:v] overlay=
			x=10:
			y=10:
			enable='between(t,1,2)'" \
		output.mp4
	'''

* **overlay multiple images** at different times
	''$ ffmpeg -i video_in.mp4 \''
		''-i image1.png -i image2.jpeg -i image3.png \''
		''-filter_complex \''
			''"[0][1]overlay=''
				''x=10:y=200:enable='between(t,655,665)'[v1]; ''
			''[v1][2]overlay=x=10:y=300:enable='between(t,1189,1190)'[v2]; ''
			''[v2][3]overlay=x=400:y=300:enable='between(t,814,820)'[v3]" \''
		''-map "[v3]" -map 0:a -c:a libx264 -crf 18 -c:a copy video_out.mp4''

* more **filter_complex** examples
	''$ ffmpeg -i myvideo.mp4 -i image.png \''
		''-filter_complex \''
		''[0:v][1:v]overlay=''
			x=''5:''
			y=''5,''
		''drawtext=text=mytext:''
			''fontcolor=orange@1.0:''
			''fontsize=30:''
			''x=30:''
			''y=200[v] \''
		''-map [v] -map 0:a -c:a copy output.mp4''
		
* **filter_complex** example to **move overlay to specific position** 
	'''
	$ ffmpeg -i input.mp4 -i image.png -y \
	       -filter_complex "[0:v][1:v]overlay=
			x='if(eq(n,439),300,0)':
			y='if(eq(n,439),300,0)':
			enable='eq(n,438)+eq(n,439)'[out]" \
	       -map [out] -map 0:a -ss 17 output.mp4
	'''

* **add a logo and simulateously text** to a video with **filter_complex**
	''$ ffmpeg -i video.mp4 -i logo.png \''
		''-filter_complex "[0:v][1:v]overlay=10:10,''
			''drawtext=text='Hello World'" \''
		''-c:a copy -movflags +faststart output.mp4''
		
* **add animated text** to a video 
	''$ ffmpeg -i input -vf \''
		''"drawtext=enable='gte(t,3)':''
		''fontfile=Vera.ttf:''
		''textfile=text.txt:''
		''reload=1:''
		''y=h-line_h-10:''
		''x=(W/tw)*n" \''
	''output''
	==> enable ''drawtext'' filter after 3 seconds
		every time ''text.txt'' is updated, text appearance will change
		text moves on screen from left to right
		reload=1		textfile will be reloaded before every frame



===== Video capture with ubuntu =====
* **screen capture** (including sound) with **recordMyDesktop**
	1. install the software
		''sudo apt-get install recordMyDesktop		'' command-prompt version//
		''sudo apt-get install gtk-recordMyDesktop'' 	 GUI version//
	2. record
		''recordmydesktop -x START_POS -y START_POS --width XSIZE --height YSIZE -o foo.ogv''
				//// e.g. recordmydesktop -x 10 -y 10 --width 640 --height 480 -o foo.ogv//
		''wininfo						'' get size and position of window//
	2a. record with the gui (set windowsize and sound)
* **video capture** using **ffmpeg** and the build-in webcam
	1. using ffmpeg w/o sound
		''ffmpeg -f video4linux2 -s 640x480 -i /dev/video0 -qscale 3 out.mpeg''
	2. using ffmpeg w/ sound
		''ffmpeg -t 00:00:15 -f video4linux2 -s 640x480 -i /dev/video0 -f alsa -ac 2 -i pulse -qscale 3 outSound.mpeg''
* **screen capture** using **ffmpeg**
	a. ''ffmpeg -f x11grab -r 25 -s 320x240 -i :0.0+10,20 out.mpg''
		''-s 320x240''		set frame size of recorded video to 320x240, shortcut for ''-video_size''
		''-r 25''			set framerate to 25, shortcut for ''-framerate''
		''-i: 0.0+10,20''		starting upper left corner of frame to record at ''(10,20)'', ''0.0'' is ''display_number.screen_number'' of X11 server, same as DISPLAY environment variable
								   one could also write:   ''-i: "${DISPLAY}.0"''
	b. ''ffmpeg -f x11grab -r 25 -s 640x480 -i :0.0 -vcodec huffyuv ffmepg_screencast.mkv''
									record with 25 framerate a window of 640x480 and with huffyuv codec
										note: will produce a HUGE file
		''ffmpeg -i in.mkv -vcodec libx264 -acodec aac -crf 22 -pix_fmt yuv420p -threads 0 out.mp4''
									--> compress to mp4 format (youtube-ready)
			''-crf''			Contant Rate Factor, for x264 range is 0-51 (depends on codec), where 0 is lossless, 23 is default, 51 is worst possible
									   lower value means higher quality
									   18 is often considered to be visually lossless
			''-threads''		setting to configure number of threads to be used, default value is 0, which tries to optimize thread usage using all available cores
			''-pix_fmt yuv420p	''default value is ''yuv444p'' which isn't compatible with all browsers (i.e. video would not be able to play in e.g. firefox)
									   if color quality seems to be somewhat reduced, try ''yuvj420p''
			  note: -vpre could be set to ensure using same presets
	c. ''ffmpeg -s 1024x768 -r 25 -f x11grab -i :0.0+100,200 output.flv''
									record 1024x768 starting with upper left corner at (100,200)
	d. ''ffmpeg -s 1024x768 -r 25 -f x11grab -i :0.0+100,200 -f alsa -ac 2 -i pulse output.flv''
									same as b. but **with sound**
			''-f alsa <input_option> -i <input_device>''
									capturing audio with ffmpeg and ALSA, <input_device> tells ffmpeg which audio capturing card or device to use
									NOTE: rebooting might change order of sound devices
									''alsamixer'' is a helpful tool to set/control audio devices
			''-f alsa -ar xxx''		audio sample rate, default value is usually "44100" (Hz)
			''-f alsa -ac x''		audio channels, default value is "1" (mono) for Mic input
	e. ''arecord -l''
									to identify correct microphone (in case of **external mic**)
		''ffmpeg -f x11grab -r 25 -s 1280x720 -i :0.0+320,236 -f alsa -i hw:1 -vcodec huffyuv lect_video.mkv''
									to use "card 1" for audio input
	f. ''ffmpeg -f x11grab -thread_queue_size 1024 -r 25 -s 1280x720 -i :0.0+320,236 -f alsa -thread_queue_size 1024 -i hw:1 -vcodec huffyuv screencast_video.mkv''
			''-thread_queue_size''	sets maximum number of queued packets when reading from the file or device. 
									   With low latency / high rate live streams, packets may be discarded if they are not read in a timely manner; raising this value can avoid it.
									   (otherwise, ''ALSA buffer xrun'' error appeared from time to time, leading to loss of audio packets)
	g. ''ffmpeg -video_size 1920x1080 -framerate 30 -f x11grab -i :0.0 -c:v libx264rgb -crf 0 -preset ultrafast output.mkv''
			''-crf 0''			tells x264 to encode in lossless mode (to reduce hardware requirements, **if CPU is not fast** enough)
			''-preset ultrafast''	do encoding fast
			''-c:v libx264rgb''		using instead ''libx264'' would be a lossy conversion from RGB to yuv444p	
	h. ''ffmpeg -f alsa -ac 2 -i hw:0,0 -f x11grab -r 30 \''
		''-s $(xwininfo -root | grep 'geometry' | awk '{print $2;}') \''
		''-i :0.0 -acodec pcm_s16le -vcodec libx264 -preset ultrafast -crf 0 -threads 0 -y screencast.avi''
			''xwininfo''		return size of window user clicks into (to define ''video_size'')
			''-y''			overwrite output file without asking
	i. ''ffmpeg -f alsa -ac 2 -i hw:0,0 -f x11grab -r 30 \''
		''-s $(xwininfo -frame | grep -oEe 'geometry [0-9]+x[0-9]+' | grep -oEe '[0-9]+x[0-9]+') \''
		''-i :0.0+$(xwininfo -frame | grep -oEe 'Corners:\s+\+[0-9]+\+[0-9]+' | grep -oEe '[0-9]+\+[0-9]+' | sed -e 's/\+/,/' ) \''
		''-acodec pcm_s16le -vcodec libx264 -preset ultrafast -crf 0 -threads 0 screencast.avi''
									record a single window, clicking mouse on window to start recording
	j. putting everything into a bash script
{{{code: lang="sh" linenumbers="True"
#!/bin/sh -

# Example values
pulseaudio_source=0
thread_queue_size=32
video_size=1920x1080
framerate=25
record_area=$DISPLAY+0,0
naudio_channels=2
outfile=capture.mkv

# Command
ffmpeg -thread_queue_size $thread_queue_size -video_size $video_size \
    -framerate $framerate -f x11grab -i $record_area \
    -thread_queue_size $thread_queue_size -f pulse -ac $naudio_channels \
    -channel_layout stereo -i $pulseaudio_source -c:v libx264 -qp 0 \
    -preset ultrafast -c:a libvorbis $outfile
}}}
		

* **screen capture** and **desktop audio capture** with ffmpeg 
	1. start recording with ffmpeg, for example
		''$ ffmpeg -f x11grab -thread_queue_size 4096 -r 25 -s 1920x1080 -i :0.0+0+0 \''
			''-f pulse -thread_queue_size 4096 -i default \''
			''-vcodec libx264 -pix_fmt yuv420p -threads 0 -acodec aac video.mp4''
	2. start ''pavucontrol'' 
		''$ pavucontrol''
	3. go to the ''Recording'' tab, you will find ''ffmpeg'' or ''Lavf57.83.100'' or similar there
	4. change audio capture from  //Built-in Audio Analog Stereo//  to  //Monitor of Built-in Audio Analog Stereo//
	5. NOTE: this setting will be remember when you start recording the next time

	Alternatively, one can enter the internal name of the monitor source manually:
		''$ pactl list sources'' 			get name of monitor source
		''$'' ''ffmpeg -f x11grab -thread_queue_size 4096 -r 25 -s 1920x1080 -i :0.0+0+0 \''
					''-f pulse -thread_queue_size 4096 -i alsa_output.pci-0000_00_1f.3.analog-stereo.monitor \''
					''-vcodec libx264 -pix_fmt yuv420p -threads 0 -acodec aac elearning_3.mp4''

	One can make it even more convenient (source: https://unix.stackexchange.com/a/489522 )
{{{code: lang="sh" linenumbers="True"
A="$(pacmd list-sources | grep -PB 1 "analog.*monitor>" | head -n 1 | perl -pe 's/.* //g')"
F="$(date --iso-8601=minutes | perl -pe 's/[^0-9]+//g').mkv"
V="$(xdpyinfo | grep dimensions | perl -pe 's/.* ([0-9]+x[0-9]+) .*/$1/g')"
ffmpeg -video_size "$V" -f x11grab -i :0.0 -f pulse -i "$A" -filter_complex amerge -ac 2 -preset veryfast "$F"

# note: for a second audio source, e.g. a microphone, one simply needs to add another '-f pulse -i default'
#       before the filter_complex (assuming that the microphone is the default audio input source) 
}}}
	
 
* some weblinks:
	* https://web2.ph.utexas.edu/~asimha/PHY303K_Spring2015/making.htm
	* https://sandilands.info/sgordon/presenting-and-recording-lectures-with-ubuntu-linux


===== Live video from webcam =====
* using **vlc**
		''$ cvlc v4l2:///dev/video0''
		''$ cvlc v4l2:///dev/video1:width=640:height=480''
* using **ffmpeg**
		? (stream to server)
* using **cheese**
		''$ sudo apt-get install cheese''
* see also
	  https://help.ubuntu.com/community/Webcam
	  http://www.netinstructions.com/tag/logitech-quickcam-pro-9000/
	  http://xmodulo.com/live-stream-video-webcam-linux.html
* **get resolution** of webcam via command line
	a. locate webcam
		$ lsusb
		   output might look like this
		   ...
		   ''Bus 003 Device 003: ID 046d:0991 Logitech, Inc. QuickCam Pro for Notebooks''
		   ...
	b. greb possible Width and Height values
		''$ lsusb -s 003:003 -v | egrep "Width|Height"''


===== Merge video files =====
* merge **avi-files**
   install required apps
	''$ sudo apt-get install transcode transcode-utils''
   merge the avi-files
	''$ avimerge -i part1.avi part2.avi -o joinedfile.avi''
* merge **mkv-files**
   install mkvtoolnix
	''$ sudo apt-get install mkvtoolnix mkvtoolnix-gui''
   Start mmg (graphical user interface for mkvmerge)
	   "Add" the first file, "Append" the second one
	   "Append" third, fourth, ..., Set output name in the textbox at the bottom
	   "Start muxing"


===== Video streaming =====

==== YouTube ====
Access the Live Streaming panel to retrieve the Live Stream URL and the Private Key to be used within the FFmpeg scripts
	1. broadcast, where video and audio are included in a single mp4 file, source: https://github.com/Darkseal/FFmpeg-scripts
{{{code: lang="sh" linenumbers="True"
#! /bin/bash
 
VBR="1500k"
FPS="30"
QUAL="ultrafast"
YOUTUBE_URL=" rtmp://a.rtmp.youtube.com/live2"
YOUTUBE_KEY="XXXXXXXXXX"
VIDEO_SOURCE="sample.mp4"
AUDIO_ENCODER="aac"

$ ffmpeg 
  -re \
  -f lavfi \
  -i "$VIDEO_SOURCE" \
  -c:v libx264 -preset $QUAL -r $FPS -g $(($FPS *2)) -b:v $VBR \
  -c:a $AUDIO_ENCODER -threads 6 -ar 44100 -b:a 128k -bufsize 512k -pix_fmt yuv420p \
  -f $YOUTUBE_URL/$YOUTUBE_KEY 
}}}
		

	2. broadcast, where video and audio are separated and then merged in broadcast, source: https://github.com/Darkseal/FFmpeg-scripts
{{{code: lang="sh" linenumbers="True"
#! /bin/bash
 
VBR="1500k"
FPS="30"
QUAL="ultrafast"
YOUTUBE_URL=" rtmp://a.rtmp.youtube.com/live2"
YOUTUBE_KEY="XXXXXXXXXX"
VIDEO_SOURCE="sample.jpg"
AUDIO_SOURCE="sample.mp3"
AUDIO_ENCODER="aac"
 
ffmpeg \
 -loop 1 \
 -re \
 -framerate $FPS \
 -i "$VIDEO_SOURCE" \
 -thread_queue_size 512 \
 -i "$AUDIO_SOURCE" \
 -c:v libx264 -preset $QUAL -r $FPS -g $(($FPS *2)) -b:v $VBR \
 -c:a $AUDIO_ENCODER -threads 6 -ar 44100 -b:a 128k -bufsize 512k -pix_fmt yuv420p \
 -f flv $YOUTUBE_URL/$YOUTUBE_KEY
}}}
		

	3. broadcast audio only, source: https://github.com/Darkseal/FFmpeg-scripts
{{{code: lang="sh" linenumbers="True"
#! /bin/bash
 
VBR="1500k"
FPS="30"
QUAL="ultrafast"
YOUTUBE_URL=" rtmp://a.rtmp.youtube.com/live2"
YOUTUBE_KEY="XXXXXXXXXX"
VIDEO_SOURCE="sample.jpg"
AUDIO_SOURCE="sample.mp3"
AUDIO_ENCODER="aac"
 
ffmpeg \
 -loop 1 \
 -re \
 -framerate $FPS \
 -i "$VIDEO_SOURCE" \
 -thread_queue_size 512 \
 -i "$AUDIO_SOURCE" \
 -c:v libx264 -tune stillimage -pix_fmt yuv420p -preset $QUAL -r $FPS -g $(($FPS *2)) -b:v $VBR \
 -c:a $AUDIO_ENCODER -threads 6 -ar 44100 -b:a 128k -bufsize 512k -pix_fmt yuv420p \
 -f flv $YOUTUBE_URL/$YOUTUBE_KEY
}}}
		


==== Twitch ====
FFmpeg has a built-in RTMP client (which is the protocol used to send video data to Twitch), FLV (wrapper for audio and video data), H.264 (video codec), and AAC (audio codec).
get RTMP ingest URL: https://stream.twitch.tv/ingests/
	''ffmpeg [input parameters] -vcodec libx264 -b:v 5M -acodec aac -b:a 256k -f flv [RTMP URL/STREAM KEY]''


==== Periscope ====
	1. broadcast, where video and audio are included in a single mp4 file, source: https://github.com/Darkseal/FFmpeg-scripts
{{{code: lang="sh" linenumbers="True"
#! /bin/bash
 
VBR="1500k"
FPS="30"
QUAL="ultrafast"
TWITTER_URL="rtmp://de.pscp.tv:80/x"
TWITTER_KEY="XXXXXXXXXX"
VIDEO_SOURCE="sample.mp4"
AUDIO_ENCODER="aac"
 
$ ffmpeg 
  -re \
  -f lavfi \
  -i "$VIDEO_SOURCE" \
  -c:v libx264 -preset $QUAL -r $FPS -g $(($FPS *2)) -b:v $VBR \
  -c:a $AUDIO_ENCODER -threads 6 -ar 44100 -b:a 128k -bufsize 512k -pix_fmt yuv420p \
  -f $TWITTER_URL/$TWITTER_KEY 
}}}
		

	2. broadcast, where video and audio are separated and then merged in broadcast, source: https://github.com/Darkseal/FFmpeg-scripts
{{{code: lang="sh" linenumbers="True"
#! /bin/bash
 
VBR="1500k"
FPS="30"
QUAL="ultrafast"
TWITTER_URL="rtmp://de.pscp.tv:80/x"
TWITTER_KEY="XXXXXXXXXX"
VIDEO_SOURCE="sample.jpg"
AUDIO_SOURCE="sample.mp3"
AUDIO_ENCODER="aac"
 
ffmpeg \
 -loop 1 \
 -re \
 -framerate $FPS \
 -i "$VIDEO_SOURCE" \
 -thread_queue_size 512 \
 -i "$AUDIO_SOURCE" \
 -c:v libx264 -preset $QUAL -r $FPS -g $(($FPS *2)) -b:v $VBR \
 -c:a $AUDIO_ENCODER -threads 6 -ar 44100 -b:a 128k -bufsize 512k -pix_fmt yuv420p \
 -f flv $TWITTER_URL/$TWITTER_KEY
}}}
		

	3. broadcast audio only, source: https://github.com/Darkseal/FFmpeg-scripts
{{{code: lang="sh" linenumbers="True"
#! /bin/bash
 
VBR="1500k"
FPS="30"
QUAL="ultrafast"
TWITTER_URL="rtmp://de.pscp.tv:80/x"
TWITTER_KEY="XXXXXXXXXX"
VIDEO_SOURCE="sample.jpg"
AUDIO_SOURCE="sample.mp3"
AUDIO_ENCODER="aac"
 
ffmpeg \
 -loop 1 \
 -re \
 -framerate $FPS \
 -i "$VIDEO_SOURCE" \
 -thread_queue_size 512 \
 -i "$AUDIO_SOURCE" \
 -c:v libx264 -tune stillimage -pix_fmt yuv420p -preset $QUAL -r $FPS -g $(($FPS *2)) -b:v $VBR \
 -c:a $AUDIO_ENCODER -threads 6 -ar 44100 -b:a 128k -bufsize 512k -pix_fmt yuv420p \
 -f flv $TWITTER_URL/$TWITTER_KEY
}}}
		

===== References =====
* ffmpeg: https://www.youtube.com/watch?v=M58rc7cxl_s
